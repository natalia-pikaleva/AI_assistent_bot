# AI-чат-бот в Telegram

## Цель: Создание интеллектуального Telegram-бота, выполняющего функции AI-ассистента,

способного обрабатывать и использовать загружаемую информацию из Google Диска
для консультаций и генерации стратегий.

## Возможности бота:

- [ ] Загрузка текстового файла с устройства или со своего Яндекс.диска
- [ ] При загрузке файлы сохраняются на Яндекс.диске приложения, каждый файл делится на фрагменты и сохраняется в базе
  знаний. Каждому фрагменту присваивается эмбендинг
- [ ] При нажатии на кнопку Поиск с AI пользователю предлагается выбрать файлы, по которым нужно искать ответ на запрос,
  можно выбрать один, несколько или все файлы.  
  Далее пользователь вводит запрос, сервис выбирает наиболее подходящий под запрос фрагменты и направляет на AI (в
  данном варианте на Gigachat), затем возвращает ответ AI пользователю
- [ ] При нажатии на кнопку Поиск в Википедии пользователь можетт ввести слово или фразу и по ним будет произведен поиск
  подходящей статьи Википедии

## Подготовка к запуску

- [ ] Клонируйте репозиторий
- [ ] Установите зависимости с помощью запуска в корне проекта команды ```pip install -r requirements.txt```
- [ ] Для работы с AI Gigachat от Сбера нужно иметь сертификат Минцифры:
- Скачайте сертификат с сайта Госуслуг: https://www.gosuslugi.ru/crt
- Установить сертификат согласно инструкции на сайте Госуслуг
- [ ] В вашей базе данных PostreSQL создайте базу ai_assistant_db
- [ ] На Яндекс.ID зарегистрируйте приложение, при регистрации укажите права доступа к любой части диска и прочие
- [ ] В корне проекта создайте файл .env, за основу возьмите шаблон .env.template:
- BOT_TOKEN - токен вашего бота, полученный от BoFather
- YANDEX_TOKEN - токен вашего приложения
- YANDEX_CLIENT_ID - Client ID
- YANDEX_CLIENT_SECRET - Client secret
- AUTHORIZATION_KEY - Authorization key
- REDIRECT_URI - ссылка на API вида "http://<хост вашего API>:<порт вашего API>/yandex_oauth_callback"
- SERTIFICAT_PATH - путь к сертификату от Минцифры вида r"path-to-certificate\russian_trusted_root_ca.cer"
- DOWNLOADS_DIR - папка, в которую временно сохраняется файл до сохранения его на Яндекс.диске и в базе данных
  Например, DOWNLOADS_DIR = "downloads"
- DATABASE_URL - адрес базы данных вида:
  DATABASE_URL = "postgresql+asyncpg://<имя пользователя>:<пароль пользователя>@<хост>:5432/ai_assistant_db"
- [ ] при первом запуске раскомментируйте в файле main.py строки, нужно загрузить один раз, затем можно закомментировать
  или удалить
  import nltk
  nltk.download('punkt')

## Запуск бота

- [ ] запустите API для обработки получения токена при авторизации на Яндекс.диске пользователем.  
  В корне проекта выполните команду ```uvicorn api:app --host 0.0.0.0 --port 8000 --reload```
- [ ] запустите бот: в корне проекта выполните команду:
  ```python main.py```

## Альтернативный вариант использования AI для бота

Любая облачная AI имеет ограниченное количество бесплатных токенов, далее нужно покупать пакеты токенов.  
Если у вас есть мощный сервер, можно развернуть AI локально и подключиться к ней

### Минимальные характеристики сервера для стабильной работы YandexGPT-5-Lite-8B-instruct-GGUF

| Компонент                 | Рекомендация                                                         |
|---------------------------|----------------------------------------------------------------------|
| **Оперативная память**    | 16 ГБ (минимум для моделей 7–8B; лучше 24–32 ГБ для многозадачности) |
| **Процессор (CPU)**       | Современный 4-ядерный и выше; желательно поддержка AVX2/AVX512       |
| **Диск**                  | SSD, свободно 12–20 ГБ под модель и рабочие файлы                    |
| **ОС**                    | Windows 10/11, Linux (Ubuntu 22.04+) или macOS                       |
| **Графика (опционально)** | GPU не требуется, но ускоряет работу (NVIDIA с поддержкой CUDA)      |

> Для запуска YandexGPT-5-Lite-8B-instruct-GGUF рекомендуется использовать сервер с не менее чем 16 ГБ оперативной
> памяти и современным процессором. SSD необходим для быстрой загрузки и работы с моделью. Для многозадачных сценариев и
> работы с большими промптами желательно 24–32 ГБ RAM. GPU может значительно ускорить инференс, но не является
> обязательным требованием

### Запуск YandexGPT-5-Lite-8B-instruct-Q4_K_M через Ollama и интеграция с Python

#### 1. Скачайте модель

- Перейдите на страницу модели (например, на Hugging Face).
- Скачайте файл `YandexGPT-5-Lite-8B-instruct-Q4_K_M.gguf`.
- Сохраните его, например, в папку:
  C:\Users<ваш_пользователь>\models\YandexGPT-5-Lite-8B-instruct-GGUF\

#### 2. Установите Ollama

- Скачайте Ollama для вашей ОС с официального сайта:  
  [https://ollama.com/download](https://ollama.com/download)
- Установите Ollama, следуя инструкциям установщика.
- Проверьте установку в терминале:
  ollama --version

- Убедитесь, что Ollama работает:  
  Откройте [http://localhost:11434](http://localhost:11434) в браузере.

#### 3. Подготовьте Modelfile

- Создайте новую папку, например:
  C:\Users<ваш_пользователь>\ollama_custom_model

- В этой папке создайте файл с именем **Modelfile** (без расширения).
- Впишите в него одну строку:
  FROM C:\Users<ваш_пользователь>\models\YandexGPT-5-Lite-8B-instruct-GGUF\YandexGPT-5-Lite-8B-instruct-Q4_K_M.gguf

#### 4. Зарегистрируйте модель в Ollama

- Откройте терминал и перейдите в папку с Modelfile:

cd C:\Users<ваш_пользователь>\ollama_custom_model

- Выполните команду:
  ollama create yandexgpt5lite -f Modelfile
  Здесь `yandexgpt5lite` — имя вашей модели в Ollama.

#### 5. Запустите модель

- В терминале выполните:
  ollama run yandexgpt5lite

- После запуска вы сможете общаться с моделью прямо в терминале.

#### 6. Используйте модель из Python

- Ollama запускает API по адресу `http://localhost:11434`.
- Пример взаимодействия с моделью из Python:

import requests

response = requests.post(  
"http://localhost:11434/api/generate",  
json={"model": "yandexgpt5lite",  
"prompt": "Привет! Объясни, как пользоваться Ollama с локальной моделью."}  
)  
print(response.json()["response"])


---

> **Требования:**
> - Минимум 16 ГБ оперативной памяти для стабильной работы модели.
> - Современный 4-ядерный процессор и SSD-диск.
> - Для многозадачности и больших промптов рекомендуется 24–32 ГБ RAM.


